{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liked</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>India is developing countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liked                                               text\n",
       "0      1                      India is developing countries\n",
       "1      1            The Da Vinci Code book is just awesome.\n",
       "2      1  this was the first clive cussler i've ever rea..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Training.txt\",sep=\"\\t\", names=['liked','text'],encoding=\"utf-8\");\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6931\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total no of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liked</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2975</td>\n",
       "      <td>559</td>\n",
       "      <td>I hate Harry Potter.</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3956</td>\n",
       "      <td>744</td>\n",
       "      <td>I love Harry Potter.</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                  \n",
       "      count unique                   top freq\n",
       "liked                                        \n",
       "0      2975    559  I hate Harry Potter.   85\n",
       "1      3956    744  I love Harry Potter.  167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('liked').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(review):\n",
    "    return TextBlob(review).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [India, is, developing, countries]\n",
       "1      [The, Da, Vinci, Code, book, is, just, awesome]\n",
       "2    [this, was, the, first, clive, cussler, i, 've...\n",
       "3             [i, liked, the, Da, Vinci, Code, a, lot]\n",
       "4             [i, liked, the, Da, Vinci, Code, a, lot]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().text.apply(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function tokens() is created to parse data/review into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(\"ready was not a good movie\")\n",
    "#nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [india, is, developing, country]\n",
       "1      [the, da, vinci, code, book, is, just, awesome]\n",
       "2    [this, wa, the, first, clive, cussler, i, 've,...\n",
       "3             [i, liked, the, da, vinci, code, a, lot]\n",
       "4             [i, liked, the, da, vinci, code, a, lot]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lemmas(review):\n",
    "    wordss = TextBlob(review.lower()).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in wordss]\n",
    "\n",
    "df.text.head().apply(to_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzr.lemmatize('octopi')\n",
    "#nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=to_lemmas).fit(df['text'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked the Da Vinci Code a lot.\n"
     ]
    }
   ],
   "source": [
    "review1=df['text'][3]\n",
    "print(review1)\n",
    "#to check 3rd document/review in collection/database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 8 stored elements and shape (1, 2114)>\n",
      "  Coords\tValues\n",
      "  (0, 42)\t1\n",
      "  (0, 369)\t1\n",
      "  (0, 458)\t1\n",
      "  (0, 950)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 1152)\t1\n",
      "  (0, 1838)\t1\n",
      "  (0, 1977)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2114)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=bow_transformer.transform([review1])\n",
    "print(bow)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-other\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names_out()[372])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (6931, 2114)\n",
      "number of non-zeros: 71297\n",
      "sparsity: 7129700.00%\n"
     ]
    }
   ],
   "source": [
    "review_bow = bow_transformer.transform(df['text'])\n",
    "print( 'sparse matrix shape:', review_bow.shape)\n",
    "print('number of non-zeros:', review_bow.nnz) #learn this\n",
    "print( 'sparsity: %.2f%%' % (100.0 * review_bow.nnz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (Term Frequency) berarti frekuensi kemunculan suatu kata dalam sebuah dokumen, sedangkan TF-IDF (Term Frequency-Inverse Document Frequency) adalah hasil perkalian antara frekuensi kata tersebut dengan nilai kebalikannya terhadap jumlah dokumen yang mengandung kata tersebut.\n",
    "\n",
    "Tujuan dari penggunaan TF-IDF dibandingkan dengan hanya menggunakan CountVectorizer adalah untuk mengurangi pengaruh kata-kata yang terlalu sering muncul dalam keseluruhan korpus (yang biasanya kurang informatif) dan meningkatkan bobot kata-kata yang jarang muncul namun lebih bermakna secara kontekstual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6931, 2114)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer =TfidfTransformer().fit(review_bow)\n",
    "review_tfidf = tfidf_transformer.transform(review_bow)\n",
    "review_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer menghasilkan output berupa frekuensi dari berbagai kata dalam korpus kita. Hasil ini kemudian diteruskan ke metode transform dari TfidfTransformer.\n",
    "\n",
    "Metode ini berfungsi untuk mengubah matriks frekuensi kata (count matrix) menjadi representasi yang sudah dinormalisasi dalam bentuk TF atau TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5544 1387 5544 1387\n"
     ]
    }
   ],
   "source": [
    "text_train, text_test, liked_train, liked_test = train_test_split(df['text'], df['liked'], test_size=0.2)\n",
    "print(len(text_train), len(text_test), len(text_train) , len(text_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang telah diunduh kemudian dibagi menjadi data pelatihan dan data pengujian dengan rasio 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=to_lemmas)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline secara berurutan menerapkan serangkaian transformasi dan estimator akhir.\n",
    "Langkah-langkah di tengah pipeline harus berupa transformasi, yaitu harus memiliki metode fit dan transform.\n",
    "Sementara estimator akhir hanya perlu memiliki metode fit.\n",
    "\n",
    "Tujuan dari pipeline adalah untuk menggabungkan beberapa tahapan pemrosesan data dan pelatihan model ke dalam satu kesatuan alur kerja, yang juga dapat divalidasi silang (cross-validation) secara bersamaan sambil menyetel berbagai parameter pada tiap tahapannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline parameters to automatically explore and tune\n",
    "param_svm = [\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm,\n",
    "    param_grid=param_svm,\n",
    "    refit=True,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pencarian menyeluruh (exhaustive search) dilakukan terhadap nilai-nilai parameter tertentu untuk suatu estimator.\n",
    "\n",
    "CV adalah singkatan dari cross-validation (validasi silang).\n",
    "Mempelajari parameter dari sebuah fungsi prediksi dan mengujinya pada data yang sama merupakan kesalahan metodologis, karena hal itu akan selalu menghasilkan akurasi 100%. Oleh karena itu, data pelatihan dan data pengujian harus dibedakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991 for {'classifier__C': 1, 'classifier__kernel': 'linear'}\n",
      "0.992 for {'classifier__C': 10, 'classifier__kernel': 'linear'}\n",
      "0.992 for {'classifier__C': 100, 'classifier__kernel': 'linear'}\n",
      "0.992 for {'classifier__C': 1000, 'classifier__kernel': 'linear'}\n",
      "0.568 for {'classifier__C': 1, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
      "0.568 for {'classifier__C': 1, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}\n",
      "0.973 for {'classifier__C': 10, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
      "0.568 for {'classifier__C': 10, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}\n",
      "0.988 for {'classifier__C': 100, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
      "0.973 for {'classifier__C': 100, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}\n",
      "0.992 for {'classifier__C': 1000, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
      "0.988 for {'classifier__C': 1000, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "for mean_score, params in zip(classifier.cv_results_['mean_test_score'], classifier.cv_results_['params']):\n",
    "    print(f\"{mean_score:.3f} for {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       579\n",
      "           1       0.99      1.00      0.99       808\n",
      "\n",
      "    accuracy                           0.99      1387\n",
      "   macro avg       0.99      0.99      0.99      1387\n",
      "weighted avg       0.99      0.99      0.99      1387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(liked_test, classifier.predict(text_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"the vinci code is awesome\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"the vinci code is bad\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32465246735834974"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussKernel(x1, x2, sigma):\n",
    "    ss=np.power(sigma,2)\n",
    "    norm= (x1-x2).T.dot(x1-x2)\n",
    "    return np.exp(-norm/(2*ss))\n",
    "x1 = np.array([1, 2, 1])\n",
    "x2 = np.array([0, 4, -1])\n",
    "sigma = 2\n",
    "gaussKernel(x1,x2,sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
